{
    "api.supers.time_createadd": {
        "code": "class supers:\n    def time_createadd(self):\n        assert self.ds.create('newsubds')\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_createadd", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "907e09f8ab0de236bf1a67890a7214a9fc1a748fd75a8286422263a49c7b9bdb", 
        "warmup_time": -1
    }, 
    "api.supers.time_createadd_to_dataset": {
        "code": "class supers:\n    def time_createadd_to_dataset(self):\n        subds = create(opj(self.ds.path, 'newsubds'))\n        self.ds.add(subds.path)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_createadd_to_dataset", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "fb42b7c0d82533916469c71544b5e2718da66403caa7a5fb45a3c17f4daeda58", 
        "warmup_time": -1
    }, 
    "api.supers.time_diff": {
        "code": "class supers:\n    def time_diff(self):\n        diff(self.ds.path, revision=\"HEAD^\")\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_diff", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "3b6661da7615e0fe8f892566ae4700081f3b882d3b76a30a0a4f555e958e3e7c", 
        "warmup_time": -1
    }, 
    "api.supers.time_diff_recursive": {
        "code": "class supers:\n    def time_diff_recursive(self):\n        diff(self.ds.path, revision=\"HEAD^\", recursive=True)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_diff_recursive", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "e234e575776bfa4b4cc34443a0497937169862e3beb3ecc00cd5fa824d4adb73", 
        "warmup_time": -1
    }, 
    "api.supers.time_installr": {
        "code": "class supers:\n    def time_installr(self):\n        # somewhat duplicating setup but lazy to do different one for now\n        assert install(self.ds.path + '_', source=self.ds.path, recursive=True)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_installr", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "44a7a6d18942c42d953dc98951a1db45d48a7c4d82e47f6eb1d35eb8cd672136", 
        "warmup_time": -1
    }, 
    "api.supers.time_ls": {
        "code": "class supers:\n    def time_ls(self):\n        ls(self.ds.path)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_ls", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "f75ae74329409bc07bdfd0f35e88090887f511a5ecafa336ce81f78f4e748888", 
        "warmup_time": -1
    }, 
    "api.supers.time_ls_recursive": {
        "code": "class supers:\n    def time_ls_recursive(self):\n        ls(self.ds.path, recursive=True)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_ls_recursive", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "032a173ca2a7a2c8e9a74b11d9c9eb69745f17ce16a5fb4eca6fee44ba4980ba", 
        "warmup_time": -1
    }, 
    "api.supers.time_ls_recursive_long_all": {
        "code": "class supers:\n    def time_ls_recursive_long_all(self):\n        ls(self.ds.path, recursive=True, long_=True, all_=True)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_ls_recursive_long_all", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "1ecc9dcc16ceab4f5d38c0d187dacc844ed042786ee7c38c3566e611c79d928b", 
        "warmup_time": -1
    }, 
    "api.supers.time_remove": {
        "code": "class supers:\n    def time_remove(self):\n        remove(self.ds.path, recursive=True)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_remove", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "8ceaec8e793ac63a9946deef100949de09291af6ae0ce283dc462a9fb979fb6a", 
        "warmup_time": -1
    }, 
    "api.supers.time_rev_createadd": {
        "code": "class supers:\n    def time_rev_createadd(self):\n        assert self.ds.rev_create('newsubds')\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_rev_createadd", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "f852e01bcb4eebc1decc936657e398592234c3a32896b3dcb7e5170df4c162ba", 
        "warmup_time": -1
    }, 
    "api.supers.time_rev_createadd_to_dataset": {
        "code": "class supers:\n    def time_rev_createadd_to_dataset(self):\n        subds = rev_create(opj(self.ds.path, 'newsubds'))\n        self.ds.rev_save(subds.path)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_rev_createadd_to_dataset", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "c12fbe32ff011e307093e91f7497e2759f357652fb1ba52c7b7fcbac941afc94", 
        "warmup_time": -1
    }, 
    "api.supers.time_status": {
        "code": "class supers:\n    def time_status(self):\n        self.ds.status()\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_status", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "d71b0d76a68bfdd9644239381e07c1f065613760f70e58ea78a07d5e363c6c96", 
        "warmup_time": -1
    }, 
    "api.supers.time_status_recursive": {
        "code": "class supers:\n    def time_status_recursive(self):\n        self.ds.status(recursive=True)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_status_recursive", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "c13a6bedcc29528f99d8dd3dcb5f47d8a9d0ca5354ee3ddd05b07902794fde29", 
        "warmup_time": -1
    }, 
    "api.supers.time_subdatasets": {
        "code": "class supers:\n    def time_subdatasets(self):\n        self.ds.subdatasets()\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_subdatasets", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "c64972840fe102038860c3bd942a2fd8c60d775290bcee41caeab2fdd95da9cf", 
        "warmup_time": -1
    }, 
    "api.supers.time_subdatasets_recursive": {
        "code": "class supers:\n    def time_subdatasets_recursive(self):\n        self.ds.subdatasets(recursive=True)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_subdatasets_recursive", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "757df20c9bb088af7bded6f6321c65122d70352c50ee5d81337c6a7d083f905a", 
        "warmup_time": -1
    }, 
    "api.supers.time_subdatasets_recursive_first": {
        "code": "class supers:\n    def time_subdatasets_recursive_first(self):\n        next(self.ds.subdatasets(recursive=True, return_type='generator'))\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_subdatasets_recursive_first", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "89c613c936c025efd13f78bcb285a84309bff4f7236566f84d2cf4614da717d6", 
        "warmup_time": -1
    }, 
    "api.supers.time_uninstall": {
        "code": "class supers:\n    def time_uninstall(self):\n        for subm in self.ds.repo.get_submodules():\n            self.ds.uninstall(subm.path, recursive=True, check=False)\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_uninstall", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "618ea6a857ea2c7aaba918592ea899d4d2a0f1be2020db904bc1b5b9edaa5676", 
        "warmup_time": -1
    }, 
    "api.testds.time_create_test_dataset1": {
        "code": "class testds:\n    def time_create_test_dataset1(self):\n        self.remove_paths.extend(\n            create_test_dataset(spec='1', seed=0)\n        )", 
        "min_run_count": 2, 
        "name": "api.testds.time_create_test_dataset1", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "28e6f5a7ca3ad723200a27ca5eb64e53d452d477915c8cc8136a5159ab464ee7", 
        "warmup_time": -1
    }, 
    "api.testds.time_create_test_dataset2x2": {
        "code": "class testds:\n    def time_create_test_dataset2x2(self):\n        self.remove_paths.extend(\n            create_test_dataset(spec='2/2', seed=0)\n        )", 
        "min_run_count": 2, 
        "name": "api.testds.time_create_test_dataset2x2", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "00cd586333d428dcd83193a76ae22af8d37bf1e444cde0916272c7e6bb25ce73", 
        "warmup_time": -1
    }, 
    "core.runner.time_echo": {
        "code": "class runner:\n    def time_echo(self):\n        self.runner.run(\"echo\")\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "min_run_count": 2, 
        "name": "core.runner.time_echo", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "5d8402946b9aefb9e96c6528161297f5924e519ff36672cad12fbee549fa82be", 
        "warmup_time": -1
    }, 
    "core.runner.time_echo_gitrunner": {
        "code": "class runner:\n    def time_echo_gitrunner(self):\n        self.git_runner.run(\"echo\")\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "min_run_count": 2, 
        "name": "core.runner.time_echo_gitrunner", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "d5e9ed1e2296b8ac7242eb11468893019ff2f49a02c4e1ee0e4f0928cc37d742", 
        "warmup_time": -1
    }, 
    "core.runner.track_overhead_100ms": {
        "code": "class runner:\n    def track_overhead_100ms(self):\n        return self._get_overhead(\"sleep 0.1\")\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_100ms", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "37e43e515d034a52c60b6b222bf98d531893ecbde6fa7c8b68b279aed94caf7e"
    }, 
    "core.runner.track_overhead_echo": {
        "code": "class runner:\n    def track_overhead_echo(self):\n        return self._get_overhead(\"echo\")\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_echo", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "b35018dec7cda8021be12fdc6c029144f7f4ffacd20bdf5abbf4884cbb922ee0"
    }, 
    "core.runner.track_overhead_heavyout": {
        "code": "class runner:\n    def track_overhead_heavyout(self):\n        # run busyloop for 100ms outputing as much as it could\n        return self._get_overhead(heavyout_cmd)\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_heavyout", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "4ca8fb7797045e550d0cc6cd204190bc2756d9388cc7add46ef9c9cd5732801d"
    }, 
    "core.runner.track_overhead_heavyout_online_process": {
        "code": "class runner:\n    def track_overhead_heavyout_online_process(self):\n        return self._get_overhead(heavyout_cmd,\n                                  log_stdout=lambda s: '',\n                                  log_stderr='offline',  # needed to would get stuck\n                                  log_online=True)\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_heavyout_online_process", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "c4043670205c71b53589e83c41caf0bd3f5c57379d46581f32fddc9f0d63a897"
    }, 
    "core.runner.track_overhead_heavyout_online_through": {
        "code": "class runner:\n    def track_overhead_heavyout_online_through(self):\n        return self._get_overhead(heavyout_cmd,\n                                  log_stderr='offline',  # needed to would get stuck\n                                  log_online=True)\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_heavyout_online_through", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "921d2f82de4ac5941b2568137e54456b22a1bab5e823acfe9969cdb1207f182f"
    }, 
    "core.startup.time_help_np": {
        "code": "class startup:\n    def time_help_np(self):\n        call([\"datalad\", \"--help-np\"], env=self.env)\n\n    def setup(self):\n        # we need to prepare/adjust PATH to point to installed datalad\n        # We will base it on taking sys.executable\n        python_path = osp.dirname(sys.executable)\n        self.env = os.environ.copy()\n        self.env['PATH'] = '%s:%s' % (python_path, self.env.get('PATH', ''))", 
        "min_run_count": 2, 
        "name": "core.startup.time_help_np", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "78c62df8b90b52ee22470b016afc051ef28ea2b4598ecaf3337d7c1d8a142479", 
        "warmup_time": -1
    }, 
    "core.startup.time_import": {
        "code": "class startup:\n    def time_import(self):\n        call([sys.executable, \"-c\", \"import datalad\"])\n\n    def setup(self):\n        # we need to prepare/adjust PATH to point to installed datalad\n        # We will base it on taking sys.executable\n        python_path = osp.dirname(sys.executable)\n        self.env = os.environ.copy()\n        self.env['PATH'] = '%s:%s' % (python_path, self.env.get('PATH', ''))", 
        "min_run_count": 2, 
        "name": "core.startup.time_import", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "5a62b417a0fe0b72410c5ac3b48be93ecac330c06565763b0209e5d0fedcc05e", 
        "warmup_time": -1
    }, 
    "core.startup.time_import_api": {
        "code": "class startup:\n    def time_import_api(self):\n        call([sys.executable, \"-c\", \"import datalad.api\"])\n\n    def setup(self):\n        # we need to prepare/adjust PATH to point to installed datalad\n        # We will base it on taking sys.executable\n        python_path = osp.dirname(sys.executable)\n        self.env = os.environ.copy()\n        self.env['PATH'] = '%s:%s' % (python_path, self.env.get('PATH', ''))", 
        "min_run_count": 2, 
        "name": "core.startup.time_import_api", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "ad7803b5cd3d3e2b45adbdbf8df9b4a3d483d0da621644a9476182740cb46246", 
        "warmup_time": -1
    }, 
    "repo.gitrepo.time_get_content_info": {
        "code": "class gitrepo:\n    def time_get_content_info(self):\n        info = self.repo.get_content_info()\n        assert isinstance(info, dict)   # just so we do not end up with a generator\n\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "repo.gitrepo.time_get_content_info", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "69b955e1615082bf213e2c51a1ff05b080130d4d2ebc19fa6b8f449275d4ad29", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_allsubmods_toplevel": {
        "code": "class get_parent_paths:\n    def time_allsubmods_toplevel(self):\n        get_parent_paths_(self.posixpaths, self.toplevel_submods)\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_allsubmods_toplevel", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "b2e359a96fbcea6335eda344c132e9bc73b24a71f22b09ad5cb722615ca5bd7f", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_allsubmods_toplevel_only": {
        "code": "class get_parent_paths:\n    def time_allsubmods_toplevel_only(self):\n        get_parent_paths_(self.posixpaths, self.toplevel_submods, True)\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_allsubmods_toplevel_only", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "f7962045a94ce196efdd3c45582fb1d3bdfa8fe0a28db4b22bbc64874695eb9c", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_no_submods": {
        "code": "class get_parent_paths:\n    def time_no_submods(self):\n        assert get_parent_paths_(self.posixpaths, [], True) == []\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_no_submods", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "599981bd1268d880b5b3dd60f95c0f0443fa2bd49fc4a87f5f3769bbeb67efdb", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_one_submod_subdir": {
        "code": "class get_parent_paths:\n    def time_one_submod_subdir(self):\n        get_parent_paths_(self.posixpaths, ['subdir/submod9'], True)\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_one_submod_subdir", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "afbb5e9229e4a9176588381966a241c894e5cfb93cdd0a088383263dd284c1a6", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_one_submod_toplevel": {
        "code": "class get_parent_paths:\n    def time_one_submod_toplevel(self):\n        get_parent_paths_(self.posixpaths, ['submod9'], True)\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_one_submod_toplevel", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "6541d79c6da5f65ecfa1c4b6bdd406f9867cb870e2083470340f12cb2c029c63", 
        "warmup_time": -1
    }, 
    "usecases.study_forrest.time_make_studyforrest_mockup": {
        "code": "class study_forrest:\n    def time_make_studyforrest_mockup(self):\n        path = self.path\n        # Carries a copy of the\n        # datalad.tests.utils_testdatasets.py:make_studyforrest_mockup\n        # as of 0.12.0rc2-76-g6ba6d53b\n        # A copy is made so we do not reflect in the benchmark results changes\n        # to that helper's code.  This copy only tests on 2 not 3 analyses\n        # subds\n        public = create(opj(path, 'public'), description=\"umbrella dataset\")\n        # the following tries to capture the evolution of the project\n        phase1 = public.create('phase1',\n                               description='old-style, no connection to RAW')\n        structural = public.create('structural', description='anatomy')\n        tnt = public.create('tnt', description='image templates')\n        tnt.clone(source=phase1.path, path=opj('src', 'phase1'), reckless=True)\n        tnt.clone(source=structural.path, path=opj('src', 'structural'), reckless=True)\n        aligned = public.create('aligned', description='aligned image data')\n        aligned.clone(source=phase1.path, path=opj('src', 'phase1'), reckless=True)\n        aligned.clone(source=tnt.path, path=opj('src', 'tnt'), reckless=True)\n        # new acquisition\n        labet = create(opj(path, 'private', 'labet'), description=\"raw data ET\")\n        phase2_dicoms = create(opj(path, 'private', 'p2dicoms'), description=\"raw data P2MRI\")\n        phase2 = public.create('phase2',\n                               description='new-style, RAW connection')\n        phase2.clone(source=labet.path, path=opj('src', 'labet'), reckless=True)\n        phase2.clone(source=phase2_dicoms.path, path=opj('src', 'dicoms'), reckless=True)\n        # add to derivatives\n        tnt.clone(source=phase2.path, path=opj('src', 'phase2'), reckless=True)\n        aligned.clone(source=phase2.path, path=opj('src', 'phase2'), reckless=True)\n        # never to be published media files\n        media = create(opj(path, 'private', 'media'), description=\"raw data ET\")\n        # assuming all annotations are in one dataset (in reality this is also\n        # a superdatasets with about 10 subdatasets\n        annot = public.create('annotations', description='stimulus annotation')\n        annot.clone(source=media.path, path=opj('src', 'media'), reckless=True)\n        # a few typical analysis datasets\n        # (just doing 2, actual status quo is just shy of 10)\n        # and also the real goal -> meta analysis\n        metaanalysis = public.create('metaanalysis', description=\"analysis of analyses\")\n        for i in range(1, 2):\n            ana = public.create('analysis{}'.format(i),\n                                description='analysis{}'.format(i))\n            ana.clone(source=annot.path, path=opj('src', 'annot'), reckless=True)\n            ana.clone(source=aligned.path, path=opj('src', 'aligned'), reckless=True)\n            ana.clone(source=tnt.path, path=opj('src', 'tnt'), reckless=True)\n            # link to metaanalysis\n            metaanalysis.clone(source=ana.path, path=opj('src', 'ana{}'.format(i)),\n                               reckless=True)\n            # simulate change in an input (but not raw) dataset\n            create_tree(\n                aligned.path,\n                {'modification{}.txt'.format(i): 'unique{}'.format(i)})\n            aligned.add('.')\n        # finally aggregate data\n        aggregate = public.create('aggregate', description='aggregate data')\n        aggregate.clone(source=aligned.path, path=opj('src', 'aligned'), reckless=True)\n\n    def setup(self):\n        self.path = tempfile.mkdtemp(**get_tempfile_kwargs({}, prefix='bm_forrest'))", 
        "min_run_count": 2, 
        "name": "usecases.study_forrest.time_make_studyforrest_mockup", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 180, 
        "type": "time", 
        "unit": "seconds", 
        "version": "f8830e56ce7099d8f41b1c386b2c23c8c434f9ee3cd3910501d1613fdbb0f18a", 
        "warmup_time": -1
    }, 
    "version": 2
}