{
    "api.supers.time_createadd": {
        "code": "class supers:\n    def time_createadd(self):\n        assert self.ds.create('newsubds')\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_createadd", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "9c578a82e573b7c77a1f2b4eab048fead5205b906cf62bb5b32378d930825b8c", 
        "warmup_time": -1
    }, 
    "api.supers.time_createadd_to_dataset": {
        "code": "class supers:\n    def time_createadd_to_dataset(self):\n        subds = create(opj(self.ds.path, 'newsubds'))\n        self.ds.add(subds.path)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_createadd_to_dataset", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "5463cdfc30b73f4d0a019facac85f2f73b6530b9a9c9714eb3050485804af1ee", 
        "warmup_time": -1
    }, 
    "api.supers.time_diff": {
        "code": "class supers:\n    def time_diff(self):\n        diff(self.ds.path, revision=\"HEAD^\")\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_diff", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "62e8272775f07fbc0053d884dddcb689c0a93347b439ecf5eea67336c62a12af", 
        "warmup_time": -1
    }, 
    "api.supers.time_diff_recursive": {
        "code": "class supers:\n    def time_diff_recursive(self):\n        diff(self.ds.path, revision=\"HEAD^\", recursive=True)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_diff_recursive", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "35e2c35d97f366be3b3165ac49a36130bdcd63c6b68599bd736010889d31b4c7", 
        "warmup_time": -1
    }, 
    "api.supers.time_installr": {
        "code": "class supers:\n    def time_installr(self):\n        # somewhat duplicating setup but lazy to do different one for now\n        assert install(self.ds.path + '_', source=self.ds.path, recursive=True)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_installr", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "480ff902ed05801249f4484e397a78ba507197f818aeb89bdacd7b153b64e369", 
        "warmup_time": -1
    }, 
    "api.supers.time_ls": {
        "code": "class supers:\n    def time_ls(self):\n        ls(self.ds.path)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_ls", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "de6b629d8d86153ab3f4712b141814f6f9a87d4f2b4283467fdb5af80d61881d", 
        "warmup_time": -1
    }, 
    "api.supers.time_ls_recursive": {
        "code": "class supers:\n    def time_ls_recursive(self):\n        ls(self.ds.path, recursive=True)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_ls_recursive", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "2386175eb95bf6a4042bb44639f31c96bec76e511be9946945daf3d1f0357678", 
        "warmup_time": -1
    }, 
    "api.supers.time_ls_recursive_long_all": {
        "code": "class supers:\n    def time_ls_recursive_long_all(self):\n        ls(self.ds.path, recursive=True, long_=True, all_=True)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_ls_recursive_long_all", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "b2b2444907bb09ade61637e4f6139caf70988269453e71b24004c526a7ba2a56", 
        "warmup_time": -1
    }, 
    "api.supers.time_remove": {
        "code": "class supers:\n    def time_remove(self):\n        remove(self.ds.path, recursive=True)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_remove", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "b9065851dbe5489d1038744bfb989ba112d9b07002fa03d428a198743377f115", 
        "warmup_time": -1
    }, 
    "api.supers.time_rev_createadd": {
        "code": "class supers:\n    def time_rev_createadd(self):\n        assert self.ds.rev_create('newsubds')\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_rev_createadd", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "27b3813fd35ee8714453f40aae5de9e1b30690d89197dfabbe085b32f8f6e867", 
        "warmup_time": -1
    }, 
    "api.supers.time_rev_createadd_to_dataset": {
        "code": "class supers:\n    def time_rev_createadd_to_dataset(self):\n        subds = rev_create(opj(self.ds.path, 'newsubds'))\n        self.ds.rev_save(subds.path)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_rev_createadd_to_dataset", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "88a25f31a3abacc0d1689808b83b2acc12f2bb6891976f27a5e2ae7c59879a7c", 
        "warmup_time": -1
    }, 
    "api.supers.time_status": {
        "code": "class supers:\n    def time_status(self):\n        self.ds.status()\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_status", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "3a1289f800cd23a8a27b87d835905c52aa15acfaf4dbc5161dcd18a39bbb684b", 
        "warmup_time": -1
    }, 
    "api.supers.time_status_recursive": {
        "code": "class supers:\n    def time_status_recursive(self):\n        self.ds.status(recursive=True)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_status_recursive", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "ac08262a480742a0975069ac54cc230142218464dbae7b2cd33920c2ba75ea32", 
        "warmup_time": -1
    }, 
    "api.supers.time_subdatasets": {
        "code": "class supers:\n    def time_subdatasets(self):\n        self.ds.subdatasets()\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_subdatasets", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "526029f407c1558262dd2da051c0531b1f7384431b590d19aae7e55b43f935d0", 
        "warmup_time": -1
    }, 
    "api.supers.time_subdatasets_recursive": {
        "code": "class supers:\n    def time_subdatasets_recursive(self):\n        self.ds.subdatasets(recursive=True)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_subdatasets_recursive", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "b8f02da631322bc7532b29d779ce4bbe27c7778230c9b3e6ad9e44fdf59395f6", 
        "warmup_time": -1
    }, 
    "api.supers.time_subdatasets_recursive_first": {
        "code": "class supers:\n    def time_subdatasets_recursive_first(self):\n        next(self.ds.subdatasets(recursive=True, return_type='generator'))\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_subdatasets_recursive_first", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "e8c6a0a20193108d2b1b2a3b469ae87e5e85fa06e41c15af1939467c51727915", 
        "warmup_time": -1
    }, 
    "api.supers.time_uninstall": {
        "code": "class supers:\n    def time_uninstall(self):\n        for subm in self.ds.repo.get_submodules():\n            self.ds.uninstall(subm.path, recursive=True, check=False)\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "api.supers.time_uninstall", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "56842c64c1fc64459b0d5287d21dac79c4b643e9a8561c30c1ae2f68029c9a65", 
        "warmup_time": -1
    }, 
    "api.testds.time_create_test_dataset1": {
        "code": "class testds:\n    def time_create_test_dataset1(self):\n        self.remove_paths.extend(\n            create_test_dataset(spec='1', seed=0)\n        )", 
        "min_run_count": 2, 
        "name": "api.testds.time_create_test_dataset1", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "28e6f5a7ca3ad723200a27ca5eb64e53d452d477915c8cc8136a5159ab464ee7", 
        "warmup_time": -1
    }, 
    "api.testds.time_create_test_dataset2x2": {
        "code": "class testds:\n    def time_create_test_dataset2x2(self):\n        self.remove_paths.extend(\n            create_test_dataset(spec='2/2', seed=0)\n        )", 
        "min_run_count": 2, 
        "name": "api.testds.time_create_test_dataset2x2", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "00cd586333d428dcd83193a76ae22af8d37bf1e444cde0916272c7e6bb25ce73", 
        "warmup_time": -1
    }, 
    "core.runner.time_echo": {
        "code": "class runner:\n    def time_echo(self):\n        self.runner.run(\"echo\")\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "min_run_count": 2, 
        "name": "core.runner.time_echo", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "5d8402946b9aefb9e96c6528161297f5924e519ff36672cad12fbee549fa82be", 
        "warmup_time": -1
    }, 
    "core.runner.time_echo_gitrunner": {
        "code": "class runner:\n    def time_echo_gitrunner(self):\n        self.git_runner.run(\"echo\")\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "min_run_count": 2, 
        "name": "core.runner.time_echo_gitrunner", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "d5e9ed1e2296b8ac7242eb11468893019ff2f49a02c4e1ee0e4f0928cc37d742", 
        "warmup_time": -1
    }, 
    "core.runner.track_overhead_100ms": {
        "code": "class runner:\n    def track_overhead_100ms(self):\n        return self._get_overhead(\"sleep 0.1\")\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_100ms", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "37e43e515d034a52c60b6b222bf98d531893ecbde6fa7c8b68b279aed94caf7e"
    }, 
    "core.runner.track_overhead_echo": {
        "code": "class runner:\n    def track_overhead_echo(self):\n        return self._get_overhead(\"echo\")\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_echo", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "b35018dec7cda8021be12fdc6c029144f7f4ffacd20bdf5abbf4884cbb922ee0"
    }, 
    "core.runner.track_overhead_heavyout": {
        "code": "class runner:\n    def track_overhead_heavyout(self):\n        # run busyloop for 100ms outputing as much as it could\n        return self._get_overhead(heavyout_cmd)\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_heavyout", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "4ca8fb7797045e550d0cc6cd204190bc2756d9388cc7add46ef9c9cd5732801d"
    }, 
    "core.runner.track_overhead_heavyout_online_process": {
        "code": "class runner:\n    def track_overhead_heavyout_online_process(self):\n        return self._get_overhead(heavyout_cmd,\n                                  log_stdout=lambda s: '',\n                                  log_stderr='offline',  # needed to would get stuck\n                                  log_online=True)\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_heavyout_online_process", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "c4043670205c71b53589e83c41caf0bd3f5c57379d46581f32fddc9f0d63a897"
    }, 
    "core.runner.track_overhead_heavyout_online_through": {
        "code": "class runner:\n    def track_overhead_heavyout_online_through(self):\n        return self._get_overhead(heavyout_cmd,\n                                  log_stderr='offline',  # needed to would get stuck\n                                  log_online=True)\n\n    def setup(self):\n        self.runner = Runner()\n        # older versions might not have it\n        try:\n            from datalad.cmd import GitRunner\n            self.git_runner = GitRunner()\n        except ImportError:\n            pass", 
        "name": "core.runner.track_overhead_heavyout_online_through", 
        "param_names": [], 
        "params": [], 
        "timeout": 60.0, 
        "type": "track", 
        "unit": "% overhead", 
        "version": "921d2f82de4ac5941b2568137e54456b22a1bab5e823acfe9969cdb1207f182f"
    }, 
    "core.startup.time_help_np": {
        "code": "class startup:\n    def time_help_np(self):\n        call([\"datalad\", \"--help-np\"], env=self.env)\n\n    def setup(self):\n        # we need to prepare/adjust PATH to point to installed datalad\n        # We will base it on taking sys.executable\n        python_path = osp.dirname(sys.executable)\n        self.env = os.environ.copy()\n        self.env['PATH'] = '%s:%s' % (python_path, self.env.get('PATH', ''))", 
        "min_run_count": 2, 
        "name": "core.startup.time_help_np", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "78c62df8b90b52ee22470b016afc051ef28ea2b4598ecaf3337d7c1d8a142479", 
        "warmup_time": -1
    }, 
    "core.startup.time_import": {
        "code": "class startup:\n    def time_import(self):\n        call([sys.executable, \"-c\", \"import datalad\"])\n\n    def setup(self):\n        # we need to prepare/adjust PATH to point to installed datalad\n        # We will base it on taking sys.executable\n        python_path = osp.dirname(sys.executable)\n        self.env = os.environ.copy()\n        self.env['PATH'] = '%s:%s' % (python_path, self.env.get('PATH', ''))", 
        "min_run_count": 2, 
        "name": "core.startup.time_import", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "5a62b417a0fe0b72410c5ac3b48be93ecac330c06565763b0209e5d0fedcc05e", 
        "warmup_time": -1
    }, 
    "core.startup.time_import_api": {
        "code": "class startup:\n    def time_import_api(self):\n        call([sys.executable, \"-c\", \"import datalad.api\"])\n\n    def setup(self):\n        # we need to prepare/adjust PATH to point to installed datalad\n        # We will base it on taking sys.executable\n        python_path = osp.dirname(sys.executable)\n        self.env = os.environ.copy()\n        self.env['PATH'] = '%s:%s' % (python_path, self.env.get('PATH', ''))", 
        "min_run_count": 2, 
        "name": "core.startup.time_import_api", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "ad7803b5cd3d3e2b45adbdbf8df9b4a3d483d0da621644a9476182740cb46246", 
        "warmup_time": -1
    }, 
    "repo.gitrepo.time_get_content_info": {
        "code": "class gitrepo:\n    def time_get_content_info(self):\n        info = self.repo.get_content_info()\n        assert isinstance(info, dict)   # just so we do not end up with a generator\n\nclass SampleSuperDatasetBenchmarks:\n    def setup(self):\n        self.log(\"Setup ran in %s, existing paths: %s\", getpwd(), glob('*'))\n    \n        tempdir = tempfile.mkdtemp(\n            **get_tempfile_kwargs({}, prefix=\"bm\")\n        )\n        self.remove_paths.append(tempdir)\n        with tarfile.open(self.tarfile) as tar:\n            tar.extractall(tempdir)\n    \n        # TODO -- remove this abomination after https://github.com/datalad/datalad/issues/1512 is fixed\n        epath = op.join(tempdir, 'testds1')\n        epath_unique = epath + str(self.__class__.ds_count)\n        os.rename(epath, epath_unique)\n        self.__class__.ds_count += 1\n        self.ds = Dataset(epath_unique)\n        self.repo = self.ds.repo\n        self.log(\"Finished setup for %s\", tempdir)\n\n    def setup_cache(self):\n        ds_path = create_test_dataset(\n            self.dsname\n            , spec='2/-2/-2'\n            , seed=0\n        )[0]\n        self.log(\"Setup cache ds path %s. CWD: %s\", ds_path, getpwd())\n        # Will store into a tarfile since otherwise install -r is way too slow\n        # to be invoked for every benchmark\n        # Store full path since apparently setup is not ran in that directory\n        self.tarfile = op.realpath(SampleSuperDatasetBenchmarks.tarfile)\n        with tarfile.open(self.tarfile, \"w\") as tar:\n            # F.CK -- Python tarfile can't later extract those because key dirs are\n            # read-only.  For now just a workaround - make it all writeable\n            from datalad.utils import rotree\n            rotree(self.dsname, ro=False, chmod_files=False)\n            tar.add(self.dsname, recursive=True)\n        rmtree(self.dsname)", 
        "min_run_count": 2, 
        "name": "repo.gitrepo.time_get_content_info", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "setup_cache_key": "/home/yoh/proj/datalad/datalad/benchmarks/common.py:123", 
        "timeout": 3600, 
        "type": "time", 
        "unit": "seconds", 
        "version": "2aac86713a30b8c039d50285718846c8ea05f72105e007b46c7f751af64a5380", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_allsubmods_toplevel": {
        "code": "class get_parent_paths:\n    def time_allsubmods_toplevel(self):\n        get_parent_paths_(self.posixpaths, self.toplevel_submods)\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_allsubmods_toplevel", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "b2e359a96fbcea6335eda344c132e9bc73b24a71f22b09ad5cb722615ca5bd7f", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_allsubmods_toplevel_only": {
        "code": "class get_parent_paths:\n    def time_allsubmods_toplevel_only(self):\n        get_parent_paths_(self.posixpaths, self.toplevel_submods, True)\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_allsubmods_toplevel_only", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "f7962045a94ce196efdd3c45582fb1d3bdfa8fe0a28db4b22bbc64874695eb9c", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_no_submods": {
        "code": "class get_parent_paths:\n    def time_no_submods(self):\n        assert get_parent_paths_(self.posixpaths, [], True) == []\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_no_submods", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "599981bd1268d880b5b3dd60f95c0f0443fa2bd49fc4a87f5f3769bbeb67efdb", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_one_submod_subdir": {
        "code": "class get_parent_paths:\n    def time_one_submod_subdir(self):\n        get_parent_paths_(self.posixpaths, ['subdir/submod9'], True)\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_one_submod_subdir", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "afbb5e9229e4a9176588381966a241c894e5cfb93cdd0a088383263dd284c1a6", 
        "warmup_time": -1
    }, 
    "support.path.get_parent_paths.time_one_submod_toplevel": {
        "code": "class get_parent_paths:\n    def time_one_submod_toplevel(self):\n        get_parent_paths_(self.posixpaths, ['submod9'], True)\n\n    def setup(self):\n        # prepare some more or less realistic with a good number of paths\n        # and some hierarchy of submodules\n        self.nfiles = 40  # per each construct\n        self.nsubmod = 30  # at two levels\n        self.toplevel_submods = ['submod%d' % i for i in range(self.nsubmod)]\n        self.posixpaths = \\\n            ['file%d' % i for i in range(self.nfiles)] + \\\n            ['subdir/anotherfile%d' % i for i in range(self.nfiles)]\n        for submod in range(self.nsubmod):\n            self.posixpaths += \\\n                ['submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['subdir/submod%d/file%d' % (submod, i) for i in range(self.nfiles)] + \\\n                ['submod/sub%d/file%d' % (submod, i) for i in range(self.nfiles)]", 
        "min_run_count": 2, 
        "name": "support.path.get_parent_paths.time_one_submod_toplevel", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 60.0, 
        "type": "time", 
        "unit": "seconds", 
        "version": "6541d79c6da5f65ecfa1c4b6bdd406f9867cb870e2083470340f12cb2c029c63", 
        "warmup_time": -1
    }, 
    "usecases.study_forrest.time_make_studyforrest_mockup": {
        "code": "class study_forrest:\n    def time_make_studyforrest_mockup(self):\n        path = self.path\n        # Carries a copy of the\n        # datalad.tests.utils_testdatasets.py:make_studyforrest_mockup\n        # as of 0.12.0rc2-76-g6ba6d53b\n        # A copy is made so we do not reflect in the benchmark results changes\n        # to that helper's code.  This copy only tests on 2 not 3 analyses\n        # subds\n        public = create(opj(path, 'public'), description=\"umbrella dataset\")\n        # the following tries to capture the evolution of the project\n        phase1 = public.create('phase1',\n                               description='old-style, no connection to RAW')\n        structural = public.create('structural', description='anatomy')\n        tnt = public.create('tnt', description='image templates')\n        tnt.clone(source=phase1.path, path=opj('src', 'phase1'), reckless=True)\n        tnt.clone(source=structural.path, path=opj('src', 'structural'), reckless=True)\n        aligned = public.create('aligned', description='aligned image data')\n        aligned.clone(source=phase1.path, path=opj('src', 'phase1'), reckless=True)\n        aligned.clone(source=tnt.path, path=opj('src', 'tnt'), reckless=True)\n        # new acquisition\n        labet = create(opj(path, 'private', 'labet'), description=\"raw data ET\")\n        phase2_dicoms = create(opj(path, 'private', 'p2dicoms'), description=\"raw data P2MRI\")\n        phase2 = public.create('phase2',\n                               description='new-style, RAW connection')\n        phase2.clone(source=labet.path, path=opj('src', 'labet'), reckless=True)\n        phase2.clone(source=phase2_dicoms.path, path=opj('src', 'dicoms'), reckless=True)\n        # add to derivatives\n        tnt.clone(source=phase2.path, path=opj('src', 'phase2'), reckless=True)\n        aligned.clone(source=phase2.path, path=opj('src', 'phase2'), reckless=True)\n        # never to be published media files\n        media = create(opj(path, 'private', 'media'), description=\"raw data ET\")\n        # assuming all annotations are in one dataset (in reality this is also\n        # a superdatasets with about 10 subdatasets\n        annot = public.create('annotations', description='stimulus annotation')\n        annot.clone(source=media.path, path=opj('src', 'media'), reckless=True)\n        # a few typical analysis datasets\n        # (just doing 2, actual status quo is just shy of 10)\n        # and also the real goal -> meta analysis\n        metaanalysis = public.create('metaanalysis', description=\"analysis of analyses\")\n        for i in range(1, 2):\n            ana = public.create('analysis{}'.format(i),\n                                description='analysis{}'.format(i))\n            ana.clone(source=annot.path, path=opj('src', 'annot'), reckless=True)\n            ana.clone(source=aligned.path, path=opj('src', 'aligned'), reckless=True)\n            ana.clone(source=tnt.path, path=opj('src', 'tnt'), reckless=True)\n            # link to metaanalysis\n            metaanalysis.clone(source=ana.path, path=opj('src', 'ana{}'.format(i)),\n                               reckless=True)\n            # simulate change in an input (but not raw) dataset\n            create_tree(\n                aligned.path,\n                {'modification{}.txt'.format(i): 'unique{}'.format(i)})\n            aligned.add('.')\n        # finally aggregate data\n        aggregate = public.create('aggregate', description='aggregate data')\n        aggregate.clone(source=aligned.path, path=opj('src', 'aligned'), reckless=True)\n\n    def setup(self):\n        self.path = tempfile.mkdtemp(**get_tempfile_kwargs({}, prefix='bm_forrest'))", 
        "min_run_count": 2, 
        "name": "usecases.study_forrest.time_make_studyforrest_mockup", 
        "number": 0, 
        "param_names": [], 
        "params": [], 
        "processes": 2, 
        "repeat": 0, 
        "sample_time": 0.01, 
        "timeout": 180, 
        "type": "time", 
        "unit": "seconds", 
        "version": "f8830e56ce7099d8f41b1c386b2c23c8c434f9ee3cd3910501d1613fdbb0f18a", 
        "warmup_time": -1
    }, 
    "version": 2
}